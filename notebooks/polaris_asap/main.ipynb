{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nepare` ASAP `polaris` Competition\n",
    "\n",
    "This notebook demonstrates using Neural Pairwise Regression (via `nepare`) with the `polaris` benchmarking library to compete in the ASAP discovery competition.\n",
    "\n",
    "## Requirements\n",
    "Python 3.10+ (originally run on 3.12)\n",
    " - polaris-lib\n",
    " - pandas\n",
    " - rdkit\n",
    " - lightning\n",
    " - torch\n",
    " - fastprop\n",
    " - chemprop 2.1\n",
    " - ipywidgets\n",
    "\n",
    "You will also need to run `pip install .` in the repository's root directory to install `nepare`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `polaris` Setup\n",
    "\n",
    "After running `polaris login` on the command line, we can import everything (checking that the version is recent enough) and then download the benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polaris as po\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "assert Version(po.__version__) >= Version(\"0.11.6\"), \"test.as_dataframe does not work in earlier versions of Polaris, please upgrade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# competition = po.load_competition(\"asap-discovery/antiviral-potency-2025\")\n",
    "# or\n",
    "competition = po.load_competition(\"asap-discovery/antiviral-admet-2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the type of embedding to use - either a fixed descriptor-based embedding with mordred (great with limited data) or a learnable embedding with ChemProp (more general representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING = \"mordred\"\n",
    "#\n",
    "# or\n",
    "# \n",
    "# EMBEDDING = \"chemprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = competition.get_train_test_split()\n",
    "test_df: pd.DataFrame = test.as_dataframe()\n",
    "train_df: pd.DataFrame = train.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Per-Task Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from chemprop.nn.agg import MeanAggregation\n",
    "from chemprop.nn.message_passing import BondMessagePassing\n",
    "from fastprop.data import standard_scale, inverse_standard_scale\n",
    "\n",
    "from nepare.data import PairwiseAugmentedDataset, PairwiseAnchoredDataset, PairwiseInferenceDataset\n",
    "from nepare.nn import LearnedEmbeddingNeuralPairwiseRegressor, NeuralPairwiseRegressor\n",
    "from nepare.inference import predict\n",
    "\n",
    "from utils.splitting import split\n",
    "from utils.chemprop_wrappers import smiles2molgraphcache, collate_fn, ChemPropEmbedder\n",
    "from utils.mordred_wrappers import smi2features\n",
    "from utils.metrics import evaluate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "tasks = list(competition.target_cols)\n",
    "dataset_kwargs = dict(batch_size=64)\n",
    "for task_n, task_name in enumerate(tasks):\n",
    "    task_df = train_df[[\"CXSMILES\", task_name]].copy()\n",
    "    task_df.dropna(inplace=True)\n",
    "    task_df.reset_index(inplace=True)\n",
    "    train_idxs, val_idxs = split(task_df[\"CXSMILES\"], test_df[\"CXSMILES\"])\n",
    "    train_targets = torch.tensor(task_df[task_name].iloc[train_idxs].to_numpy(), dtype=torch.float32).reshape(-1, 1)  # 2d!\n",
    "    train_targets, target_means, target_vars = standard_scale(train_targets)\n",
    "    val_targets = torch.tensor(task_df[task_name].iloc[val_idxs].to_numpy(), dtype=torch.float32).reshape(-1, 1)  # 2d!\n",
    "    val_targets = standard_scale(val_targets, target_means, target_vars)\n",
    "    if EMBEDDING == \"mordred\":\n",
    "        Model = NeuralPairwiseRegressor\n",
    "        train_features, feature_means, feature_vars = smi2features(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_features, _, _ = smi2features(task_df[\"CXSMILES\"][val_idxs], feature_means, feature_vars)\n",
    "        test_features, _, _ = smi2features(test_df[\"CXSMILES\"], feature_means, feature_vars)\n",
    "        # setup the datasets\n",
    "        train_dataset = PairwiseAugmentedDataset(train_features, train_targets, how='full')\n",
    "        val_dataset = PairwiseAnchoredDataset(train_features, train_targets, val_features, val_targets, how='full')\n",
    "        val_absolute_dataset = PairwiseInferenceDataset(train_features, train_targets, val_features, how='full')\n",
    "        test_dataset = PairwiseInferenceDataset(train_features, train_targets, test_features, how='full')\n",
    "        # build the model\n",
    "        npr = Model(train_features.shape[1], 100, 3)\n",
    "    elif EMBEDDING == \"chemprop\":\n",
    "        Model = LearnedEmbeddingNeuralPairwiseRegressor\n",
    "        # featurize the data\n",
    "        train_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][val_idxs])\n",
    "        test_mgc = smiles2molgraphcache(test_df[\"CXSMILES\"])\n",
    "        # setup the datasets\n",
    "        dataset_kwargs[\"collate_fn\"] = collate_fn\n",
    "        train_dataset = PairwiseAugmentedDataset(train_mgc, train_targets, how='sut')\n",
    "        val_dataset = PairwiseAnchoredDataset(train_mgc, train_targets, val_mgc, val_targets, how='half')\n",
    "        val_absolute_dataset = PairwiseInferenceDataset(train_mgc, train_targets, val_mgc, how='half')\n",
    "        test_dataset = PairwiseInferenceDataset(train_mgc, train_targets, test_mgc, how='half')\n",
    "        # build the model\n",
    "        mp = BondMessagePassing(d_h=400)\n",
    "        agg = MeanAggregation()\n",
    "        embedder = ChemPropEmbedder(mp, agg)\n",
    "        npr = Model(embedder, 400, 200, 2, lr=5e-5)\n",
    "\n",
    "    # classic lightning training, inference\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **dataset_kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, **dataset_kwargs)\n",
    "    val_absolute_loader = torch.utils.data.DataLoader(val_absolute_dataset, **dataset_kwargs)\n",
    "    predict_loader = torch.utils.data.DataLoader(test_dataset, **dataset_kwargs)\n",
    "    early_stopping = EarlyStopping(monitor=\"validation/loss\", patience=3)\n",
    "    model_checkpoint = ModelCheckpoint(monitor=\"validation/loss\")\n",
    "    trainer = lightning.Trainer(max_epochs=50, log_every_n_steps=1, callbacks=[early_stopping, model_checkpoint])\n",
    "    trainer.fit(npr, train_loader, val_loader)\n",
    "    npr = Model.load_from_checkpoint(model_checkpoint.best_model_path)\n",
    "    y_pred, y_stdev = predict(npr, val_absolute_loader)\n",
    "    y_pred = inverse_standard_scale(y_pred, target_means, target_vars)\n",
    "    predictions[task_name] = y_pred.flatten().tolist()\n",
    "\n",
    "    # show the validation performance for our own information\n",
    "    print(f\"{task_name=}: {\"\\n\".join([f'{name}: {score:.4f}' for name, score in evaluate_predictions(val_targets.flatten().numpy(), y_pred.flatten().numpy()).items()])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last block is commented out because it will fail (unless you are me) - you can replace the inputs with your own if you are submitting this for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition.submit_predictions(\n",
    "#     predictions=predictions,\n",
    "#     prediction_name=\"nepare_chemprop\",\n",
    "#     prediction_owner=\"jacksonburns\",\n",
    "#     report_url=\"https://github.com/JacksonBurns/neural-pairwise-regression/blob/main/meta\",\n",
    "#     github_url = \"https://github.com/JacksonBurns/neural-pairwise-regression/blob/main/notebooks/nepare_asap.ipynb\",\n",
    "#     description = \"Neural Pairwise Regression with ChemProp as a learnable embedding\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
