{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nepare` ASAP `polaris` Competition\n",
    "\n",
    "This notebook demonstrates using Neural Pairwise Regression (via `nepare`) with the `polaris` benchmarking library to compete in the ASAP discovery competition.\n",
    "\n",
    "## Requirements\n",
    "Python 3.10+ (originally run on 3.12)\n",
    " - polaris-lib\n",
    " - pandas\n",
    " - rdkit\n",
    " - lightning\n",
    " - torch\n",
    " - fastprop\n",
    " - chemprop 2.1\n",
    " - ipywidgets\n",
    "\n",
    "You will also need to run `pip install .` in the repository's root directory to install `nepare`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `polaris` Setup\n",
    "\n",
    "After running `polaris login` on the command line, we can import everything (checking that the version is recent enough) and then download the benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polaris as po\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "competition = po.load_competition(\"asap-discovery/antiviral-potency-2025\")\n",
    "# or\n",
    "# competition = po.load_competition(\"asap-discovery/antiviral-admet-2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the type of embedding to use - either a fixed descriptor-based embedding with mordred (great with limited data) or a learnable embedding with ChemProp (more general representation).\n",
    "The choice as shown is configured based on previous experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_m, _c, _b = \"mordred\", \"chemprop\", \"combined\"\n",
    "EMBEDDING = {\n",
    "    'HLM': _c,\n",
    "    'MLM': _b,\n",
    "    'KSOL': _m,\n",
    "    'LogD': _m,\n",
    "    'MDR1-MDCKII': _m,\n",
    "    'pIC50 (MERS-CoV Mpro)': _b,\n",
    "    'pIC50 (SARS-CoV-2 Mpro)': _b,\n",
    "}\n",
    "# could consider a combined ebedding where the dataset returns two tuples (0 is chemprop features, 1 is mordred) and then a slightly modified learnable_ebedding module that does what the current chemprop one does except conatenates the output with mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = competition.get_train_test_split()\n",
    "test_df: pd.DataFrame = test.as_dataframe()\n",
    "train_df: pd.DataFrame = train.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Per-Task Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import lightning\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from chemprop.nn.agg import MeanAggregation\n",
    "from chemprop.nn.message_passing import BondMessagePassing\n",
    "from fastprop.data import standard_scale, inverse_standard_scale\n",
    "\n",
    "from nepare.data import PairwiseAugmentedDataset, PairwiseAnchoredDataset, PairwiseInferenceDataset\n",
    "from nepare.nn import LearnedEmbeddingNeuralPairwiseRegressor, NeuralPairwiseRegressor\n",
    "from nepare.inference import predict\n",
    "\n",
    "from utils.splitting import split\n",
    "from utils.chemprop_wrappers import smiles2molgraphcache, collate_fn, ChemPropEmbedder\n",
    "from utils.mordred_wrappers import smi2features\n",
    "from utils.metrics import evaluate_predictions\n",
    "from utils.combined_embeddings import MordredChemPropEmbedder, collate_fn_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "val_score = []\n",
    "output_dir = Path(\"lightning_logs\")\n",
    "tasks = list(competition.target_cols)\n",
    "for task_n, task_name in enumerate(tasks):\n",
    "    dataset_kwargs = dict(batch_size=64)\n",
    "    task_df = train_df[[\"CXSMILES\", task_name]].copy()\n",
    "    task_df.dropna(inplace=True)\n",
    "    task_df.reset_index(inplace=True)\n",
    "    train_idxs, val_idxs = split(task_df[\"CXSMILES\"], test_df[\"CXSMILES\"])\n",
    "    train_targets = torch.tensor(task_df[task_name].iloc[train_idxs].to_numpy(), dtype=torch.float32).reshape(-1, 1)  # 2d!\n",
    "    train_targets, target_means, target_vars = standard_scale(train_targets)\n",
    "    val_targets = torch.tensor(task_df[task_name].iloc[val_idxs].to_numpy(), dtype=torch.float32).reshape(-1, 1)  # 2d!\n",
    "    val_targets = standard_scale(val_targets, target_means, target_vars)\n",
    "    if EMBEDDING[task_name] == \"mordred\":\n",
    "        Model = NeuralPairwiseRegressor\n",
    "        train_features, feature_means, feature_vars = smi2features(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_features, _, _ = smi2features(task_df[\"CXSMILES\"][val_idxs], feature_means, feature_vars)\n",
    "        test_features, _, _ = smi2features(test_df[\"CXSMILES\"], feature_means, feature_vars)\n",
    "        # setup the datasets\n",
    "        train_dataset = PairwiseAugmentedDataset(train_features, train_targets, how='full')\n",
    "        val_dataset = PairwiseAnchoredDataset(train_features, train_targets, val_features, val_targets, how='full')\n",
    "        val_absolute_dataset = PairwiseInferenceDataset(train_features, train_targets, val_features, how='full')\n",
    "        test_dataset = PairwiseInferenceDataset(train_features, train_targets, test_features, how='full')\n",
    "        # build the model\n",
    "        npr = Model(train_features.shape[1], 70, 3)\n",
    "    elif EMBEDDING[task_name] == \"chemprop\":\n",
    "        Model = LearnedEmbeddingNeuralPairwiseRegressor\n",
    "        # featurize the data\n",
    "        train_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][val_idxs])\n",
    "        test_mgc = smiles2molgraphcache(test_df[\"CXSMILES\"])\n",
    "        # setup the datasets\n",
    "        dataset_kwargs[\"collate_fn\"] = collate_fn\n",
    "        train_dataset = PairwiseAugmentedDataset(train_mgc, train_targets, how='sut')\n",
    "        val_dataset = PairwiseAnchoredDataset(train_mgc, train_targets, val_mgc, val_targets, how='half')\n",
    "        val_absolute_dataset = PairwiseInferenceDataset(train_mgc, train_targets, val_mgc, how='half')\n",
    "        test_dataset = PairwiseInferenceDataset(train_mgc, train_targets, test_mgc, how='half')\n",
    "        # build the model\n",
    "        mp = BondMessagePassing(d_h=400)\n",
    "        agg = MeanAggregation()\n",
    "        embedder = ChemPropEmbedder(mp, agg)\n",
    "        npr = Model(embedder, 400, 200, 2, lr=5e-5)\n",
    "    elif EMBEDDING[task_name] == \"combined\":\n",
    "        Model = LearnedEmbeddingNeuralPairwiseRegressor\n",
    "        # featurize the data\n",
    "        train_features, feature_means, feature_vars = smi2features(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_features, _, _ = smi2features(task_df[\"CXSMILES\"][val_idxs], feature_means, feature_vars)\n",
    "        test_features, _, _ = smi2features(test_df[\"CXSMILES\"], feature_means, feature_vars)\n",
    "        train_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][train_idxs])\n",
    "        val_mgc = smiles2molgraphcache(task_df[\"CXSMILES\"][val_idxs])\n",
    "        test_mgc = smiles2molgraphcache(test_df[\"CXSMILES\"])\n",
    "        train_tuples = [(train_mgc[i], train_features[i, :]) for i in range(len(train_targets))]\n",
    "        val_tuples = [(val_mgc[i], val_features[i, :]) for i in range(len(val_targets))]\n",
    "        test_tuples = [(test_mgc[i], test_features[i, :]) for i in range(len(test_mgc))]\n",
    "        # setup the datasets\n",
    "        train_dataset = PairwiseAugmentedDataset(train_tuples, train_targets, how='full')\n",
    "        val_dataset = PairwiseAnchoredDataset(train_tuples, train_targets, val_tuples, val_targets, how='full')\n",
    "        val_absolute_dataset = PairwiseInferenceDataset(train_tuples, train_targets, val_tuples, how='full')\n",
    "        test_dataset = PairwiseInferenceDataset(train_tuples, train_targets, test_tuples, how='full')\n",
    "        dataset_kwargs[\"collate_fn\"] = collate_fn_combined\n",
    "        # build the model\n",
    "        mp = BondMessagePassing(d_h=1_000, depth=2)\n",
    "        agg = MeanAggregation()\n",
    "        embedder = MordredChemPropEmbedder(mp, agg)\n",
    "        npr = Model(embedder, mp.output_dim + train_features.shape[1], 1_000, 2, lr=5e-5)\n",
    "\n",
    "\n",
    "    # classic lightning training, inference\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **dataset_kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, **dataset_kwargs)\n",
    "    val_absolute_loader = torch.utils.data.DataLoader(val_absolute_dataset, **dataset_kwargs)\n",
    "    predict_loader = torch.utils.data.DataLoader(test_dataset, **dataset_kwargs)\n",
    "    early_stopping = EarlyStopping(monitor=\"validation/loss\", patience=3)\n",
    "    name = \"_\".join([EMBEDDING[task_name], task_name])\n",
    "    model_checkpoint = ModelCheckpoint(dirpath=output_dir / name, monitor=\"validation/loss\")\n",
    "    logger = TensorBoardLogger(save_dir=output_dir, name=name, default_hp_metric=False)\n",
    "    trainer = lightning.Trainer(max_epochs=50, log_every_n_steps=1, callbacks=[early_stopping, model_checkpoint], logger=logger)\n",
    "    trainer.fit(npr, train_loader, val_loader)\n",
    "    npr = Model.load_from_checkpoint(model_checkpoint.best_model_path)\n",
    "    y_pred, y_stdev = predict(npr, val_absolute_loader)\n",
    "    # leave in the scaled space so that the average is weighted equally among the targets\n",
    "    results_dict = evaluate_predictions(val_targets.flatten().numpy(), y_pred.flatten().numpy())\n",
    "    val_score.append((len(val_targets), results_dict[\"MAE\"]))\n",
    "    # now go back to correct scale\n",
    "    y_pred = inverse_standard_scale(y_pred, target_means, target_vars)\n",
    "    y_true = inverse_standard_scale(val_targets, target_means, target_vars)\n",
    "\n",
    "    # metric reporting, for our own information\n",
    "    print(f\"{task_name=}:\\n - {\"\\n - \".join([f'{name}: {score:.4f}' for name, score in evaluate_predictions(y_true.flatten().numpy(), y_pred.flatten().numpy()).items()])}\")\n",
    "\n",
    "    # actual predictions\n",
    "    y_pred, y_stdev = predict(npr, predict_loader)\n",
    "    y_pred = inverse_standard_scale(y_pred, target_means, target_vars)\n",
    "    predictions[task_name] = y_pred.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation Scaled Weighted Average MAE {sum(n*s for n, s in val_score) / sum(n for n, _ in val_score):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last block is commented out because it will fail (unless you are me) - you can replace the inputs with your own if you are submitting this for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition.submit_predictions(\n",
    "#     predictions=predictions,\n",
    "#     prediction_name=\"nepare\",\n",
    "#     prediction_owner=\"jacksonburns\",\n",
    "#     report_url=\"https://github.com/JacksonBurns/neural-pairwise-regression/blob/main/meta\",\n",
    "#     github_url = \"https://github.com/JacksonBurns/neural-pairwise-regression/blob/main/notebooks/polaris_asap/main.ipynb\",\n",
    "#     description = \"Neural Pairwise Regression with ChemProp as a learnable embedding\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
